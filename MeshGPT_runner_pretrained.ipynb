{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install  -q git+https://github.com/MarcusLoppe/meshgpt-pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'model_M0001'\n",
    "working_dir = f'{project_name}'\n",
    "models_dir = f'{working_dir}/models'\n",
    "dataset_dir = f'{working_dir}/datasets'\n",
    "model_name = f\"M0000_E11\"\n",
    "\n",
    "encoder_checkpoint = f'{models_dir}/2024-07-23-M0001-encoder-loss_0.170.pt'\n",
    "transformer_checkpoint = f'{models_dir}/mesh-transformer.ckpt.epoch_0_avg_loss_0.903.pt'\n",
    "dataset_path = f\"{dataset_dir}/objverse_shapenet_modelnet_max_250faces_186M_tokens.npz\"\n",
    "\n",
    "# To extract labels from dataset\n",
    "#from meshgpt_pytorch import MeshDataset \n",
    "#dataset = MeshDataset.load(dataset_path)#, map_location=torch.device('cpu')) \n",
    "#labels = list(set(item[\"texts\"] for item in dataset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from meshgpt_pytorch import (\n",
    "    MeshTransformerTrainer,\n",
    "    MeshAutoencoderTrainer,\n",
    "    MeshAutoencoder,\n",
    "    MeshTransformer\n",
    ")\n",
    "from meshgpt_pytorch.data import ( \n",
    "    derive_face_edges_from_faces\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 50.7M\n"
     ]
    }
   ],
   "source": [
    "# 16k 2 4\n",
    "autoencoder = MeshAutoencoder(     \n",
    "    decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,   \n",
    "    dim_codebook = 192,  \n",
    "    dim_area_embed = 16,\n",
    "    dim_coor_embed = 16, \n",
    "    dim_normal_embed = 16,\n",
    "    dim_angle_embed = 8,    \n",
    "    attn_decoder_depth  = 4,\n",
    "    attn_encoder_depth = 2\n",
    ").to(\"cpu\")\n",
    "\n",
    "total_params = sum(p.numel() for p in autoencoder.parameters()) \n",
    "total_params = f\"{total_params / 1000000:.1f}M\"\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Trained on a dataset of 14k models that contains less then 250 faces (objverse + shapenet + Modelnet).\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Reached the loss of 0.399 MSE\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pkg \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_checkpoint\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mload_state_dict(pkg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m autoencoder\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m~/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/_utils.py:114\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# \n",
    "# Trained on a dataset of 14k models that contains less then 250 faces (objverse + shapenet + Modelnet).\n",
    "# Reached the loss of 0.399 MSE\n",
    "pkg = torch.load(encoder_checkpoint) \n",
    "autoencoder.load_state_dict(pkg['model'])\n",
    "for param in autoencoder.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder total parameters: 321.5M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc  \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()   \n",
    "# max_seq = max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)  * (autoencoder.num_vertices_per_face * autoencoder.num_quantizers) \n",
    "# print(\"Max token sequence:\" , max_seq)  \n",
    "transformer = MeshTransformer(\n",
    "    autoencoder,\n",
    "    dim =768,\n",
    "    coarse_pre_gateloop_depth = 6,  \n",
    "    fine_pre_gateloop_depth= 4, \n",
    "    attn_depth = 24,  \n",
    "    attn_heads = 16,\n",
    "    dropout  = 0.0,\n",
    "    max_seq_len = 1500,\n",
    "    condition_on_text = True, \n",
    "    gateloop_use_heinsen = False,\n",
    "    text_condition_model_types = \"bge\", \n",
    "    text_condition_cond_drop_prob = 0.0, \n",
    ").to(\"cpu\") \n",
    "\n",
    "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
    "total_params = f\"{total_params / 1000000:.1f}M\"\n",
    "print(f\"Decoder total parameters: {total_params}\")\n",
    "\n",
    "pkg = torch.load(transformer_checkpoint, map_location=torch.device('cpu')) \n",
    "transformer.load_state_dict(pkg['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and view mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using only text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "`text` or `text_embeds` must be passed in if `condition_on_text` is set to True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m obj_file_path\u001b[38;5;241m.\u001b[39mmkdir(exist_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, parents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[1;32m      9\u001b[0m text_coords \u001b[38;5;241m=\u001b[39m [] \n\u001b[0;32m---> 10\u001b[0m text_coords\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m)   \n\u001b[1;32m     12\u001b[0m current_datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m results_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_datetime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_test_results.obj\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Teo/Projects/meshgpt-pytorch/.venv/lib/python3.12/site-packages/x_transformers/autoregressive_wrapper.py:29\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m was_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 29\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(was_training)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/Teo/Projects/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Teo/Projects/meshgpt-pytorch/meshgpt_pytorch/meshgpt_pytorch.py:1337\u001b[0m, in \u001b[0;36mMeshTransformer.generate\u001b[0;34m(self, prompt, batch_size, filter_logits_fn, filter_kwargs, temperature, return_codes, texts, text_embeds, cond_scale, cache_kv, max_seq_len, face_coords_to_file)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcondition_on_text:\n\u001b[0;32m-> 1337\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exists(texts) \u001b[38;5;241m^\u001b[39m exists(text_embeds), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`text` or `text_embeds` must be passed in if `condition_on_text` is set to True\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exists(texts):\n\u001b[1;32m   1339\u001b[0m         text_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_texts(texts)\n",
      "\u001b[0;31mAssertionError\u001b[0m: `text` or `text_embeds` must be passed in if `condition_on_text` is set to True"
     ]
    }
   ],
   "source": [
    "from meshgpt_pytorch import mesh_render \n",
    "from pathlib import Path\n",
    "import datetime\n",
    " \n",
    "folder = f'{working_dir}/renders'\n",
    "obj_file_path = Path(folder)\n",
    "obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
    "\n",
    "text_coords = [] \n",
    "text_coords.append(transformer.generate(texts = [text],  temperature = 0.0))   \n",
    "\n",
    "current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "results_filename = f\"{current_datetime}_{model_name}_test_results.obj\"\n",
    "mesh_render.save_rendering(f'{folder}/{results_filename}', text_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# from meshgpt_pytorch import mesh_render \n",
    "# from pathlib import Path\n",
    "# import datetime\n",
    " \n",
    "# folder = f'{working_dir}/renders'\n",
    "# obj_file_path = Path(folder)\n",
    "# obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
    "\n",
    "# query = [\n",
    "#     'tv table', 'office table', 'high chair', 'glass table',\n",
    "#     'designer sloped chair', 'designer chair', 'corner table', 'circle chair', 'bar chair',\n",
    "#     'shoe', 'cup', 'plate', 'vase', 'person'\n",
    "# ]\n",
    "    \n",
    "# text_coords = [] \n",
    "# for text in (query):\n",
    "#     print(f\"Generating {text}\") \n",
    "#     text_coords.append(transformer.generate(texts = [text],  temperature = 0.0))   \n",
    "\n",
    "# current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# results_filename = f\"{current_datetime}_{model_name}_test_results.obj\"\n",
    "# mesh_render.save_rendering(f'{folder}/{results_filename}', text_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text + prompt of tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt with 10% of codes/tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path \n",
    "# from meshgpt_pytorch import mesh_render \n",
    "# folder = working_dir / f'renders/text+codes'\n",
    "# obj_file_path = Path(folder)\n",
    "# obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
    "\n",
    "# token_length_procent = 0.10 \n",
    "# codes = []\n",
    "# texts = []\n",
    "# for label in labels:\n",
    "#     for item in dataset.data: \n",
    "#         if item['texts'] == label:\n",
    "#             tokens = autoencoder.tokenize(\n",
    "#                 vertices = item['vertices'],\n",
    "#                 faces = item['faces'],\n",
    "#                 face_edges = item['face_edges']\n",
    "#             ) \n",
    "#             num_tokens = int(tokens.shape[0] * token_length_procent)  \n",
    "#             texts.append(item['texts']) \n",
    "#             codes.append(tokens.flatten()[:num_tokens].unsqueeze(0))  \n",
    "#             break\n",
    "        \n",
    "# coords = []  \n",
    "# for text, prompt in zip(texts, codes): \n",
    "#     print(f\"Generating {text} with {prompt.shape[1]} tokens\") \n",
    "#     coords.append(transformer.generate(texts = [text],  prompt = prompt, temperature = 0) )    \n",
    "      \n",
    "# mesh_render.save_rendering(f'{folder}/text+prompt_{token_length_procent*100}.obj', coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt with 0% to 80% of tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from meshgpt_pytorch import mesh_render \n",
    " \n",
    "# folder = working_dir / f'renders/text+codes_rows'\n",
    "# obj_file_path = Path(folder)\n",
    "# obj_file_path.mkdir(exist_ok = True, parents = True)   \n",
    "\n",
    "# mesh_rows = []\n",
    "# for token_length_procent in np.arange(0, 0.8, 0.1):\n",
    "#     codes = []\n",
    "#     texts = []\n",
    "#     for label in labels:\n",
    "#         for item in dataset.data: \n",
    "#             if item['texts'] == label:\n",
    "#                 tokens = autoencoder.tokenize(\n",
    "#                     vertices = item['vertices'],\n",
    "#                     faces = item['faces'],\n",
    "#                     face_edges = item['face_edges']\n",
    "#                 ) \n",
    "#                 num_tokens = int(tokens.shape[0] * token_length_procent) \n",
    "                \n",
    "#                 texts.append(item['texts']) \n",
    "#                 codes.append(tokens.flatten()[:num_tokens].unsqueeze(0))  \n",
    "#                 break\n",
    "            \n",
    "#     coords = []   \n",
    "#     for text, prompt in zip(texts, codes):  \n",
    "#         print(f\"Generating {text} with {prompt.shape[1]} tokens\") \n",
    "#         coords.append(transformer.generate(texts = [text],  prompt = prompt, temperature = 0)) \n",
    "         \n",
    "#     mesh_rows.append(coords)  \n",
    "    \n",
    "# mesh_render.save_rendering(f'{folder}/all.obj', mesh_rows)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just some testing for text embedding similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# texts = list(labels)\n",
    "# vectors = [transformer.conditioner.text_models[0].embed_text([text], return_text_encodings = False).cpu().flatten() for text in texts]\n",
    " \n",
    "# max_label_length = max(len(text) for text in texts)\n",
    " \n",
    "# # Print the table header\n",
    "# print(f\"{'Text':<{max_label_length}} |\", end=\" \")\n",
    "# for text in texts:\n",
    "#     print(f\"{text:<{max_label_length}} |\", end=\" \")\n",
    "# print()\n",
    "\n",
    "# # Print the similarity matrix as a table with fixed-length columns\n",
    "# for i in range(len(texts)):\n",
    "#     print(f\"{texts[i]:<{max_label_length}} |\", end=\" \")\n",
    "#     for j in range(len(texts)):\n",
    "#         # Encode the texts and calculate cosine similarity manually\n",
    "#         vector_i = vectors[i]\n",
    "#         vector_j = vectors[j]\n",
    "        \n",
    "#         dot_product = torch.sum(vector_i * vector_j)\n",
    "#         norm_vector1 = torch.norm(vector_i)\n",
    "#         norm_vector2 = torch.norm(vector_j)\n",
    "#         similarity_score = dot_product / (norm_vector1 * norm_vector2)\n",
    "        \n",
    "#         # Print with fixed-length columns\n",
    "#         print(f\"{similarity_score.item():<{max_label_length}.4f} |\", end=\" \")\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
