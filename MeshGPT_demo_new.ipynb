{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (1.26.4)\n",
                  "Requirement already satisfied: trimesh in ./.venv/lib/python3.12/site-packages (4.3.2)\n",
                  "Requirement already satisfied: numpy>=1.20 in ./.venv/lib/python3.12/site-packages (from trimesh) (1.26.4)\n"
               ]
            }
         ],
         "source": [
            "!pip install  -q git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
            "!pip install numpy\n",
            "!pip install trimesh"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/Users/matteo/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            }
         ],
         "source": [
            "import torch\n",
            "torch.set_default_device(\"mps\")\n",
            "import trimesh\n",
            "import numpy as np\n",
            "import os\n",
            "import csv\n",
            "import json\n",
            "from collections import OrderedDict\n",
            "\n",
            "from meshgpt_pytorch import (\n",
            "    MeshTransformerTrainer,\n",
            "    MeshAutoencoderTrainer,\n",
            "    MeshAutoencoder,\n",
            "    MeshTransformer\n",
            ")\n",
            "from meshgpt_pytorch.data import ( \n",
            "    derive_face_edges_from_faces\n",
            ") \n",
            "\n",
            "def get_mesh(file_path): \n",
            "    mesh = trimesh.load(file_path, force='mesh') \n",
            "    vertices = mesh.vertices.tolist()\n",
            "    if \".off\" in file_path:  # ModelNet dataset\n",
            "       mesh.vertices[:, [1, 2]] = mesh.vertices[:, [2, 1]] \n",
            "       rotation_matrix = trimesh.transformations.rotation_matrix(np.radians(-90), [0, 1, 0])\n",
            "       mesh.apply_transform(rotation_matrix) \n",
            "        # Extract vertices and faces from the rotated mesh\n",
            "       vertices = mesh.vertices.tolist()\n",
            "            \n",
            "    faces = mesh.faces.tolist()\n",
            "    \n",
            "    centered_vertices = vertices - np.mean(vertices, axis=0)  \n",
            "    max_abs = np.max(np.abs(centered_vertices))\n",
            "    vertices = centered_vertices / (max_abs / 0.95)     # Limit vertices to [-0.95, 0.95]\n",
            "      \n",
            "    min_y = np.min(vertices[:, 1]) \n",
            "    difference = -0.95 - min_y \n",
            "    vertices[:, 1] += difference\n",
            "    \n",
            "    def sort_vertices(vertex):\n",
            "        return vertex[1], vertex[2], vertex[0]   \n",
            " \n",
            "    seen = OrderedDict()\n",
            "    for point in vertices: \n",
            "      key = tuple(point)\n",
            "      if key not in seen:\n",
            "        seen[key] = point\n",
            "        \n",
            "    unique_vertices =  list(seen.values()) \n",
            "    sorted_vertices = sorted(unique_vertices, key=sort_vertices)\n",
            "      \n",
            "    vertices_as_tuples = [tuple(v) for v in vertices]\n",
            "    sorted_vertices_as_tuples = [tuple(v) for v in sorted_vertices]\n",
            "\n",
            "    vertex_map = {old_index: new_index for old_index, vertex_tuple in enumerate(vertices_as_tuples) for new_index, sorted_vertex_tuple in enumerate(sorted_vertices_as_tuples) if vertex_tuple == sorted_vertex_tuple} \n",
            "    reindexed_faces = [[vertex_map[face[0]], vertex_map[face[1]], vertex_map[face[2]]] for face in faces] \n",
            "    sorted_faces = [sorted(sub_arr) for sub_arr in reindexed_faces]   \n",
            "    return np.array(sorted_vertices), np.array(sorted_faces)\n",
            " \n",
            " \n",
            "\n",
            "def augment_mesh(vertices, scale_factor):     \n",
            "    jitter_factor=0.01 \n",
            "    possible_values = np.arange(-jitter_factor, jitter_factor , 0.0005) \n",
            "    offsets = np.random.choice(possible_values, size=vertices.shape) \n",
            "    vertices = vertices + offsets   \n",
            "    \n",
            "    vertices = vertices * scale_factor \n",
            "    # To ensure that the mesh models are on the \"ground\"\n",
            "    min_y = np.min(vertices[:, 1])  \n",
            "    difference = -0.95 - min_y \n",
            "    vertices[:, 1] += difference\n",
            "    return vertices\n",
            "\n",
            "\n",
            "#load_shapenet(\"./shapenet\", \"./shapenet_csv_files\", 10, 10)   \n",
            "#Find the csv files with the labels in the ShapeNetCore.v1.zip, download at  https://huggingface.co/datasets/ShapeNet/ShapeNetCore-archive  \n",
            "def load_shapenet(directory, per_category, variations ):\n",
            "    obj_datas = []   \n",
            "    chosen_models_count = {}    \n",
            "    print(f\"per_category: {per_category} variations {variations}\")\n",
            "    \n",
            "    with open('shapenet_labels.json' , 'r') as f:\n",
            "        id_info = json.load(f) \n",
            "    \n",
            "    possible_values = np.arange(0.75, 1.0 , 0.005) \n",
            "    scale_factors = np.random.choice(possible_values, size=variations) \n",
            "    \n",
            "    for category in os.listdir(directory): \n",
            "        category_path = os.path.join(directory, category)   \n",
            "        if os.path.isdir(category_path) == False:\n",
            "            continue \n",
            "        \n",
            "        num_files_in_category = len(os.listdir(category_path))\n",
            "        print(f\"{category_path} got {num_files_in_category} files\") \n",
            "        chosen_models_count[category] = 0  \n",
            "        \n",
            "        for filename in os.listdir(category_path):\n",
            "            if filename.endswith((\".obj\", \".glb\", \".off\")):\n",
            "                file_path = os.path.join(category_path, filename)\n",
            "                \n",
            "                if chosen_models_count[category] >= per_category:\n",
            "                    break \n",
            "                if os.path.getsize(file_path) >  20 * 1024: # 20 kb limit = less then 400-600 faces\n",
            "                    continue \n",
            "                if filename[:-4] not in id_info:\n",
            "                    print(\"Unable to find id info for \", filename)\n",
            "                    continue \n",
            "                vertices, faces = get_mesh(file_path) \n",
            "                if len(faces) > 800: \n",
            "                    continue\n",
            "                \n",
            "                chosen_models_count[category] += 1  \n",
            "                textName = id_info[filename[:-4]]   \n",
            "                \n",
            "                face_edges =  derive_face_edges_from_faces(faces)  \n",
            "                for scale_factor in scale_factors: \n",
            "                    aug_vertices = augment_mesh(vertices.copy(), scale_factor)   \n",
            "                    obj_data = {\"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"mps\"), \"faces\":  torch.tensor(faces.tolist(), dtype=torch.long).to(\"mps\"), \"face_edges\" : face_edges, \"texts\": textName }  \n",
            "                    obj_datas.append(obj_data)\n",
            "                    \n",
            "    print(\"=\"*25)\n",
            "    print(\"Chosen models count for each category:\")\n",
            "    for category, count in chosen_models_count.items():\n",
            "        print(f\"{category}: {count}\") \n",
            "    total_chosen_models = sum(chosen_models_count.values())\n",
            "    print(f\"Total number of chosen models: {total_chosen_models}\")\n",
            "    return obj_datas\n",
            "\n",
            "  \n",
            "   \n",
            "def load_filename(directory, variations):\n",
            "    obj_datas = []    \n",
            "    possible_values = np.arange(0.75, 1.0 , 0.005) \n",
            "    scale_factors = np.random.choice(possible_values, size=variations) \n",
            "    \n",
            "    for filename in os.listdir(directory):\n",
            "        if filename.endswith((\".obj\", \".glb\", \".off\")): \n",
            "            file_path = os.path.join(directory, filename) \n",
            "            vertices, faces = get_mesh(file_path)  \n",
            "            \n",
            "            faces = torch.tensor(faces.tolist(), dtype=torch.long).to(\"mps\")\n",
            "            face_edges =  derive_face_edges_from_faces(faces)  \n",
            "            texts, ext = os.path.splitext(filename)     \n",
            "            \n",
            "            for scale_factor in scale_factors: \n",
            "                aug_vertices = augment_mesh(vertices.copy(), scale_factor)  \n",
            "                obj_data = {\"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"mps\"), \"faces\":  faces, \"face_edges\" : face_edges, \"texts\": texts } \n",
            "                obj_datas.append(obj_data)\n",
            "                     \n",
            "    print(f\"[create_mesh_dataset] Returning {len(obj_data)} meshes\")\n",
            "    return obj_datas"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "<>:9: SyntaxWarning: invalid escape sequence '\\p'\n",
                  "<>:9: SyntaxWarning: invalid escape sequence '\\p'\n",
                  "/var/folders/cx/v3j2fvp11zn8ncnw1z7zdxgw0000gn/T/ipykernel_44595/878580155.py:9: SyntaxWarning: invalid escape sequence '\\p'\n",
                  "  pali_captions = pd.read_csv('.\\pali_captions.csv', sep=';') # https://github.com/google-deepmind/objaverse_annotations/blob/main/pali_captions.csv\n"
               ]
            }
         ],
         "source": [
            "import gzip,json\n",
            "from tqdm import tqdm\n",
            "import pandas as pd\n",
            "\n",
            "# Instruction to download objverse meshes: https://github.com/MarcusLoppe/Objaverse-downloader/tree/main\n",
            "def load_objverse(directory, variations ):\n",
            "    obj_datas = []     \n",
            "    id_info = {}   \n",
            "    pali_captions = pd.read_csv('.\\pali_captions.csv', sep=';') # https://github.com/google-deepmind/objaverse_annotations/blob/main/pali_captions.csv\n",
            "    pali_captions_dict = pali_captions.set_index(\"object_uid\").to_dict()[\"top_aggregate_caption\"]  \n",
            "        \n",
            "    possible_values = np.arange(0.75, 1.0) \n",
            "    scale_factors = np.random.choice(possible_values, size=variations) \n",
            "    \n",
            "    for folder in os.listdir(directory):  \n",
            "        full_folder_path = os.path.join(directory, folder)   \n",
            "        if os.path.isdir(full_folder_path) == False:\n",
            "            continue    \n",
            "         \n",
            "        for filename in tqdm(os.listdir(full_folder_path)):  \n",
            "            if filename.endswith((\".obj\", \".glb\", \".off\")):\n",
            "                file_path = os.path.join(full_folder_path, filename)\n",
            "                kb = os.path.getsize(file_path)  / 1024 \n",
            "                if kb < 1 or kb > 30:\n",
            "                    continue\n",
            "                  \n",
            "                if filename[:-4] not in pali_captions_dict: \n",
            "                    continue   \n",
            "                textName =  pali_captions_dict[filename[:-4]]\n",
            "                try:    \n",
            "                    vertices, faces = get_mesh(file_path)   \n",
            "                except Exception as e:\n",
            "                    continue\n",
            "                \n",
            "                if len(faces) > 250 or len(faces) < 50: \n",
            "                    continue\n",
            "                \n",
            "                faces = torch.tensor(faces.tolist(), dtype=torch.long).to(\"mps\")\n",
            "                face_edges = derive_face_edges_from_faces(faces)   \n",
            "                for scale_factor in scale_factors: \n",
            "                    aug_vertices = augment_mesh(vertices.copy(), scale_factor)   \n",
            "                    obj_data = {\"filename\": filename, \"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"mps\"), \"faces\":  faces, \"face_edges\" : face_edges, \"texts\": textName }   \n",
            "                    obj_datas.append(obj_data)  \n",
            "    return obj_datas"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "<string>:8: SyntaxWarning: invalid escape sequence '\\{'\n",
                  "<>:8: SyntaxWarning: invalid escape sequence '\\{'\n",
                  "<string>:8: SyntaxWarning: invalid escape sequence '\\{'\n",
                  "<>:8: SyntaxWarning: invalid escape sequence '\\{'\n",
                  "/var/folders/cx/v3j2fvp11zn8ncnw1z7zdxgw0000gn/T/ipykernel_44595/4011593429.py:8: SyntaxWarning: invalid escape sequence '\\{'\n",
                  "  working_dir = f'.\\{project_name}'\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[create_mesh_dataset] Returning 4 meshes\n",
                  "[MeshDataset] Created from 450 entrys\n",
                  "[MeshDataset] Generated face_edges for 0/450 entrys\n",
                  "[MeshDataset] Saved 450 entrys at .\\demo_mesh/demo_mesh.npz\n",
                  "[MeshDataset] Loaded 450 entrys\n",
                  "[MeshDataset] Created from 450 entrys\n",
                  "dict_keys(['vertices', 'faces', 'face_edges', 'texts'])\n"
               ]
            }
         ],
         "source": [
            "from pathlib import Path \n",
            "import gc     \n",
            "import os\n",
            "from meshgpt_pytorch import MeshDataset \n",
            " \n",
            "project_name = \"demo_mesh\" \n",
            "\n",
            "working_dir = f'.\\{project_name}'\n",
            "\n",
            "working_dir = Path(working_dir)\n",
            "working_dir.mkdir(exist_ok = True, parents = True)\n",
            "dataset_path = working_dir / (project_name + \".npz\")\n",
            "\n",
            "torch.set_default_device(\"mps\")\n",
            "if not os.path.isfile(dataset_path):\n",
            "    data = load_filename(\"./demo_mesh\",50)  \n",
            "    dataset = MeshDataset(data) \n",
            "    dataset.generate_face_edges()  \n",
            "    dataset.save(dataset_path)\n",
            " \n",
            "dataset = MeshDataset.load(dataset_path) \n",
            "print(dataset.data[0].keys())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Inspect imported meshes (optional)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            "\n",
            "working_dir = '/Users/matteo/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/out'\n",
            "folder = working_dir + '/renders' \n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
            "   \n",
            "all_vertices = []\n",
            "all_faces = []\n",
            "vertex_offset = 0\n",
            "translation_distance = 0.5  \n",
            "\n",
            "for r, item in enumerate(data): \n",
            "    \n",
            "    vertices_copy =  np.copy(item['vertices'].cpu())\n",
            "    vertices_copy += translation_distance * (r / 0.2 - 1) \n",
            "    \n",
            "    for vert in vertices_copy:\n",
            "        vertex = vert #vert.to('cpu')\n",
            "        all_vertices.append(f\"v {float(vertex[0])}  {float(vertex[1])}  {float(vertex[2])}\\n\") \n",
            "    for face in item['faces']:\n",
            "        all_faces.append(f\"f {face[0]+1+ vertex_offset} {face[1]+ 1+vertex_offset} {face[2]+ 1+vertex_offset}\\n\")  \n",
            "    vertex_offset = len(all_vertices)\n",
            " \n",
            "obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
            " \n",
            "obj_file_path = f'{folder}/3d_models_inspect.obj' \n",
            "with open(obj_file_path, \"w\") as file:\n",
            "    file.write(obj_file_content)    \n",
            "    "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "PosixPath('.\\\\demo_mesh/renders')"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "folder"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Train!"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Total parameters: 50.7M\n"
               ]
            }
         ],
         "source": [
            "autoencoder = MeshAutoencoder(      \n",
            "        decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,   \n",
            "        codebook_size = 2048,  # Smaller vocab size will speed up the transformer training, however if you are training on meshes more then 250 triangle, I'd advice to use 16384 codebook size\n",
            "        dim_codebook = 192,  \n",
            "        dim_area_embed = 16,\n",
            "        dim_coor_embed = 16, \n",
            "        dim_normal_embed = 16,\n",
            "        dim_angle_embed = 8,\n",
            "    \n",
            "    attn_decoder_depth  = 4,\n",
            "    attn_encoder_depth = 2\n",
            ").to(\"mps\")     \n",
            "total_params = sum(p.numel() for p in autoencoder.parameters()) \n",
            "total_params = f\"{total_params / 1000000:.1f}M\"\n",
            "print(f\"Total parameters: {total_params}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "45000\n"
               ]
            }
         ],
         "source": [
            "dataset.data = [dict(d) for d in dataset.data] * 10\n",
            "print(len(dataset.data))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "*Load previous saved model if you had to restart session*"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [
            {
               "ename": "FileNotFoundError",
               "evalue": "[Errno 2] No such file or directory: '/Users/matteo/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/out/mesh-encoder_demo_mesh.pt'",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pkg \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mworking_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/mesh-encoder_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproject_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      2\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mload_state_dict(pkg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m autoencoder\u001b[38;5;241m.\u001b[39mparameters():\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
                  "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/matteo/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/out/mesh-encoder_demo_mesh.pt'"
               ]
            }
         ],
         "source": [
            "pkg = torch.load(str(f'{working_dir}/mesh-encoder_{project_name}.pt')) \n",
            "autoencoder.load_state_dict(pkg['model'])\n",
            "for param in autoencoder.parameters():\n",
            "    param.requires_grad = True"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Train to about 0.3 loss if you are using a small dataset**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Epoch 1/480:  15%|█▌        | 427/2812 [05:58<33:19,  1.19it/s, commit_loss=76.8, loss=20.5, recon_loss=5.12]\n"
               ]
            },
            {
               "ename": "KeyboardInterrupt",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcommit_loss_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;66;03m# Set dependant on the dataset size, on smaller datasets, 0.1 is fine, otherwise try from 0.25 to 0.4.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m autoencoder_trainer \u001b[38;5;241m=\u001b[39m MeshAutoencoderTrainer(model \u001b[38;5;241m=\u001b[39mautoencoder ,warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, dataset \u001b[38;5;241m=\u001b[39m dataset, num_train_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                              batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      9\u001b[0m                                              grad_accum_every \u001b[38;5;241m=\u001b[39m grad_accum_every,\n\u001b[1;32m     10\u001b[0m                                              learning_rate \u001b[38;5;241m=\u001b[39m learning_rate,\n\u001b[1;32m     11\u001b[0m                                              checkpoint_every_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstop_at_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiplay_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m        \n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/meshgpt_pytorch/trainer.py:369\u001b[0m, in \u001b[0;36mMeshAutoencoderTrainer.train\u001b[0;34m(self, num_epochs, stop_at_loss, diplay_graph)\u001b[0m\n\u001b[1;32m    366\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mcurrent_loss, recon_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(recon_loss\u001b[38;5;241m.\u001b[39mitem(),\u001b[38;5;241m3\u001b[39m), commit_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(commit_loss\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem(),\u001b[38;5;241m4\u001b[39m)) \n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_last \u001b[38;5;129;01mor\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader)): \n\u001b[0;32m--> 369\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[1;32m    374\u001b[0m avg_recon_loss \u001b[38;5;241m=\u001b[39m total_epoch_recon_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader)\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/pytorch_custom_utils/optimizer_scheduler_warmup.py:77\u001b[0m, in \u001b[0;36mOptimizerWithWarmupSchedule.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(param_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped:\n\u001b[1;32m     80\u001b[0m     context \u001b[38;5;241m=\u001b[39m nullcontext \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarmup) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarmup\u001b[38;5;241m.\u001b[39mdampening\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/accelerate/optimizer.py:170\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/optim/adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    443\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
                  "File \u001b[0;32m~/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/.venv/lib/python3.12/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
               ]
            }
         ],
         "source": [
            "batch_size=16 # The batch size should be max 64.\n",
            "grad_accum_every = 4\n",
            "# So set the maximal batch size (max 64) that your VRAM can handle and then use grad_accum_every to create a effective batch size of 64, e.g  16 * 4 = 64\n",
            "learning_rate = 1e-3 # Start with 1e-3 then at staggnation around 0.35, you can lower it to 1e-4.\n",
            "\n",
            "autoencoder.commit_loss_weight = 0.2 # Set dependant on the dataset size, on smaller datasets, 0.1 is fine, otherwise try from 0.25 to 0.4.\n",
            "autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
            "                                             batch_size=batch_size,\n",
            "                                             grad_accum_every = grad_accum_every,\n",
            "                                             learning_rate = learning_rate,\n",
            "                                             checkpoint_every_epoch=1) \n",
            "autoencoder_trainer.load('./checkpoints/mesh-autoencoder.ckpt.epoch_0_avg_loss_7.35620_recon_4.7973_commit_12.7944.pt')\n",
            "loss = autoencoder_trainer.train(480,stop_at_loss = 0.2, diplay_graph= True)        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [],
         "source": [
            "autoencoder_trainer.save(f'{working_dir}/mesh-encoder_{project_name}.pt')   "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Inspect how the autoencoder can encode and then provide the decoder with the codes to reconstruct the mesh"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "import random\n",
            "from tqdm import tqdm \n",
            "from meshgpt_pytorch import mesh_render \n",
            "\n",
            "min_mse, max_mse = float('inf'), float('-inf')\n",
            "min_coords, min_orgs, max_coords, max_orgs = None, None, None, None\n",
            "random_samples, random_samples_pred, all_random_samples = [], [], []\n",
            "total_mse, sample_size = 0.0, 200\n",
            "\n",
            "random.shuffle(dataset.data)\n",
            "\n",
            "for item in tqdm(dataset.data[:sample_size]):\n",
            "    codes = autoencoder.tokenize(vertices=item['vertices'], faces=item['faces'], face_edges=item['face_edges']) \n",
            "    \n",
            "    codes = codes.flatten().unsqueeze(0)\n",
            "    codes = codes[:, :codes.shape[-1] // autoencoder.num_quantizers * autoencoder.num_quantizers] \n",
            " \n",
            "    coords, mask = autoencoder.decode_from_codes_to_faces(codes)\n",
            "    orgs = item['vertices'][item['faces']].unsqueeze(0)\n",
            "\n",
            "    mse = torch.mean((orgs.view(-1, 3).cpu() - coords.view(-1, 3).cpu())**2)\n",
            "    total_mse += mse \n",
            "\n",
            "    if mse < min_mse: min_mse, min_coords, min_orgs = mse, coords, orgs\n",
            "    if mse > max_mse: max_mse, max_coords, max_orgs = mse, coords, orgs\n",
            " \n",
            "    if len(random_samples) <= 30:\n",
            "        random_samples.append(coords)\n",
            "        random_samples_pred.append(orgs)\n",
            "    else:\n",
            "        all_random_samples.extend([random_samples_pred, random_samples])\n",
            "        random_samples, random_samples_pred = [], []\n",
            "\n",
            "print(f'MSE AVG: {total_mse / sample_size:.10f}, Min: {min_mse:.10f}, Max: {max_mse:.10f}')    \n",
            "mesh_render.combind_mesh_with_rows(f'{working_dir}\\mse_rows.obj', all_random_samples)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Training & fine-tuning\n",
            "\n",
            "**Pre-train:** Train the transformer on the full dataset with all the augmentations, the longer / more epochs will create a more robust model.<br/>\n",
            "\n",
            "**Fine-tune:** Since it will take a long time to train on all the possible augmentations of the meshes, I recommend that you remove all the augmentations so you are left with x1 model per mesh.<br/>\n",
            "Below is the function **filter_dataset** that will return a single copy of each mesh.<br/>\n",
            "The function can also check for duplicate labels, this may speed up the fine-tuning process (not recommanded) however this most likely will remove it's ability for novel mesh generation."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import gc  \n",
            "torch.cuda.empty_cache()\n",
            "gc.collect()   \n",
            "max_seq = max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)  * (autoencoder.num_vertices_per_face * autoencoder.num_quantizers) \n",
            "print(\"Max token sequence:\" , max_seq)  \n",
            "\n",
            "# GPT2-Small model\n",
            "transformer = MeshTransformer(\n",
            "    autoencoder,\n",
            "    dim = 768,\n",
            "    coarse_pre_gateloop_depth = 3,  \n",
            "    fine_pre_gateloop_depth= 3,  \n",
            "    attn_depth = 12,  \n",
            "    attn_heads = 12,  \n",
            "    max_seq_len = max_seq, \n",
            "    condition_on_text = True, \n",
            "    gateloop_use_heinsen = False,\n",
            "    dropout  = 0.0,\n",
            "    text_condition_model_types = \"bge\", \n",
            "    text_condition_cond_drop_prob = 0.0\n",
            ") \n",
            "\n",
            "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
            "total_params = f\"{total_params / 1000000:.1f}M\"\n",
            "print(f\"Decoder total parameters: {total_params}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def filter_dataset(dataset, unique_labels = False):\n",
            "    unique_dicts = []\n",
            "    unique_tensors = set()\n",
            "    texts = set()\n",
            "    for d in dataset.data:\n",
            "        tensor = d[\"faces\"]\n",
            "        tensor_tuple = tuple(tensor.cpu().numpy().flatten())\n",
            "        if unique_labels and d['texts'] in texts:\n",
            "            continue\n",
            "        if tensor_tuple not in unique_tensors:\n",
            "            unique_tensors.add(tensor_tuple)\n",
            "            unique_dicts.append(d)\n",
            "            texts.add(d['texts'])\n",
            "    return unique_dicts \n",
            "#dataset.data = filter_dataset(dataset.data, unique_labels = False)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
            "\n",
            "**If you don't;** <br>\n",
            "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
            "<br>\n",
            "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
            "\n",
            "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "labels = list(set(item[\"texts\"] for item in dataset.data))\n",
            "dataset.embed_texts(transformer, batch_size = 25)\n",
            "dataset.generate_codes(autoencoder, batch_size = 50)\n",
            "print(dataset.data[0].keys())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "*Load previous saved model if you had to restart session*"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pkg = torch.load(str(f'{working_dir}\\mesh-transformer_{project_name}.pt')) \n",
            "transformer.load_state_dict(pkg['model'])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "batch_size = 4 # Max 64\n",
            "grad_accum_every = 16\n",
            "\n",
            "# Set the maximal batch size (max 64) that your VRAM can handle and then use grad_accum_every to create a effective batch size of 64, e.g  4 * 16 = 64\n",
            "learning_rate = 1e-2 # Start training with the learning rate at 1e-2 then lower it to 1e-3 at stagnation or at 0.5 loss.\n",
            "\n",
            "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,num_train_steps=100, dataset = dataset,\n",
            "                                 grad_accum_every=grad_accum_every,\n",
            "                                 learning_rate = learning_rate,\n",
            "                                 batch_size=batch_size,\n",
            "                                 checkpoint_every_epoch = 1,\n",
            "                                 # FP16 training, it doesn't speed up very much but can increase the batch size which will in turn speed up the training.\n",
            "                                 # However it might cause nan after a while.\n",
            "                                 # accelerator_kwargs = {\"mixed_precision\" : \"fp16\"}, optimizer_kwargs = { \"eps\": 1e-7} \n",
            "                                 )\n",
            "loss = trainer.train(300, stop_at_loss = 0.005)  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.save(f'{working_dir}\\mesh-transformer_{project_name}.pt')   "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Generate and view mesh"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Using only text**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            " \n",
            "from meshgpt_pytorch import mesh_render \n",
            "from pathlib import Path\n",
            " \n",
            "folder = working_dir / 'renders'\n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
            " \n",
            "text_coords = [] \n",
            "for text in labels[:10]:\n",
            "    print(f\"Generating {text}\")\n",
            "    faces_coordinates = transformer.generate(texts = [text],  temperature = 0.0) \n",
            "    text_coords.append(faces_coordinates)   \n",
            "mesh_render.combind_mesh(f'{folder}/3d_models_all.obj', text_coords)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Text + prompt of tokens**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Prompt with 10% of codes/tokens**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path \n",
            "from meshgpt_pytorch import mesh_render \n",
            "folder = working_dir / f'renders/text+codes'\n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
            "\n",
            "token_length_procent = 0.10 \n",
            "codes = []\n",
            "texts = []\n",
            "for label in labels:\n",
            "    for item in dataset.data: \n",
            "        if item['texts'] == label:\n",
            "            tokens = autoencoder.tokenize(\n",
            "                vertices = item['vertices'],\n",
            "                faces = item['faces'],\n",
            "                face_edges = item['face_edges']\n",
            "            ) \n",
            "            num_tokens = int(tokens.shape[0] * token_length_procent)  \n",
            "            texts.append(item['texts']) \n",
            "            codes.append(tokens.flatten()[:num_tokens].unsqueeze(0))  \n",
            "            break\n",
            "        \n",
            "coords = []  \n",
            "for text, prompt in zip(texts, codes): \n",
            "    print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
            "    faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0) \n",
            "    coords.append(faces_coordinates)   \n",
            "    print(obj_file_path)\n",
            "      \n",
            "mesh_render.combind_mesh(f'{folder}/text+prompt_{token_length_procent*100}.obj', coords)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Prompt with 0% to 80% of tokens**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            "from meshgpt_pytorch import mesh_render \n",
            " \n",
            "folder = working_dir / f'renders/text+codes_rows'\n",
            "obj_file_path = Path(folder)\n",
            "obj_file_path.mkdir(exist_ok = True, parents = True)   \n",
            "\n",
            "mesh_rows = []\n",
            "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
            "    codes = []\n",
            "    texts = []\n",
            "    for label in labels:\n",
            "        for item in dataset.data: \n",
            "            if item['texts'] == label:\n",
            "                tokens = autoencoder.tokenize(\n",
            "                    vertices = item['vertices'],\n",
            "                    faces = item['faces'],\n",
            "                    face_edges = item['face_edges']\n",
            "                ) \n",
            "                num_tokens = int(tokens.shape[0] * token_length_procent) \n",
            "                \n",
            "                texts.append(item['texts']) \n",
            "                codes.append(tokens.flatten()[:num_tokens].unsqueeze(0))  \n",
            "                break\n",
            "            \n",
            "    coords = []   \n",
            "    for text, prompt in zip(texts, codes):  \n",
            "        print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
            "        faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0) \n",
            "        coords.append(faces_coordinates) \n",
            "         \n",
            "    mesh_rows.append(coords) \n",
            "    mesh_render.combind_mesh(f'{folder}/text+prompt_all_{token_length_procent*100}.obj', coords)\n",
            "    \n",
            "mesh_render.combind_mesh_with_rows(f'{folder}/all.obj', mesh_rows)\n",
            " "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Just some testing for text embedding similarity**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np \n",
            "texts = list(labels)\n",
            "vectors = [transformer.conditioner.text_models[0].embed_text([text], return_text_encodings = False).cpu().flatten() for text in texts]\n",
            " \n",
            "max_label_length = max(len(text) for text in texts)\n",
            " \n",
            "# Print the table header\n",
            "print(f\"{'Text':<{max_label_length}} |\", end=\" \")\n",
            "for text in texts:\n",
            "    print(f\"{text:<{max_label_length}} |\", end=\" \")\n",
            "print()\n",
            "\n",
            "# Print the similarity matrix as a table with fixed-length columns\n",
            "for i in range(len(texts)):\n",
            "    print(f\"{texts[i]:<{max_label_length}} |\", end=\" \")\n",
            "    for j in range(len(texts)):\n",
            "        # Encode the texts and calculate cosine similarity manually\n",
            "        vector_i = vectors[i]\n",
            "        vector_j = vectors[j]\n",
            "        \n",
            "        dot_product = torch.sum(vector_i * vector_j)\n",
            "        norm_vector1 = torch.norm(vector_i)\n",
            "        norm_vector2 = torch.norm(vector_j)\n",
            "        similarity_score = dot_product / (norm_vector1 * norm_vector2)\n",
            "        \n",
            "        # Print with fixed-length columns\n",
            "        print(f\"{similarity_score.item():<{max_label_length}.4f} |\", end=\" \")\n",
            "    print()"
         ]
      }
   ],
   "metadata": {
      "kaggle": {
         "accelerator": "gpu",
         "dataSources": [],
         "dockerImageVersionId": 30627,
         "isGpuEnabled": true,
         "isInternetEnabled": true,
         "language": "python",
         "sourceType": "notebook"
      },
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.3"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
