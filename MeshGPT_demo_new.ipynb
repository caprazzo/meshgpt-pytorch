{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -q git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
    "!pip install numpy\n",
    "!pip install trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cmartella/Projects/meshgpt-pytorch-kite/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_default_device(\"mps\")\n",
    "import trimesh\n",
    "import PIL\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from meshgpt_pytorch import (\n",
    "    MeshTransformerTrainer,\n",
    "    MeshAutoencoderTrainer,\n",
    "    MeshAutoencoder,\n",
    "    MeshTransformer,\n",
    "    MultiModalEmbeddingReturner\n",
    ")\n",
    "from meshgpt_pytorch.data import ( \n",
    "    derive_face_edges_from_faces\n",
    ") \n",
    "\n",
    "def get_mesh(file_path): \n",
    "    mesh = trimesh.load(file_path, force='mesh') \n",
    "    vertices = mesh.vertices.tolist()\n",
    "    if \".off\" in file_path:  # ModelNet dataset\n",
    "       mesh.vertices[:, [1, 2]] = mesh.vertices[:, [2, 1]] \n",
    "       rotation_matrix = trimesh.transformations.rotation_matrix(np.radians(-90), [0, 1, 0])\n",
    "       mesh.apply_transform(rotation_matrix) \n",
    "        # Extract vertices and faces from the rotated mesh\n",
    "       vertices = mesh.vertices.tolist()\n",
    "            \n",
    "    faces = mesh.faces.tolist()\n",
    "    \n",
    "    centered_vertices = vertices - np.mean(vertices, axis=0)  \n",
    "    max_abs = np.max(np.abs(centered_vertices))\n",
    "    vertices = centered_vertices / (max_abs / 0.95)     # Limit vertices to [-0.95, 0.95]\n",
    "      \n",
    "    min_y = np.min(vertices[:, 1]) \n",
    "    difference = -0.95 - min_y \n",
    "    vertices[:, 1] += difference\n",
    "    \n",
    "    def sort_vertices(vertex):\n",
    "        return vertex[1], vertex[2], vertex[0]   \n",
    " \n",
    "    seen = OrderedDict()\n",
    "    for point in vertices: \n",
    "      key = tuple(point)\n",
    "      if key not in seen:\n",
    "        seen[key] = point\n",
    "        \n",
    "    unique_vertices =  list(seen.values()) \n",
    "    sorted_vertices = sorted(unique_vertices, key=sort_vertices)\n",
    "      \n",
    "    vertices_as_tuples = [tuple(v) for v in vertices]\n",
    "    sorted_vertices_as_tuples = [tuple(v) for v in sorted_vertices]\n",
    "\n",
    "    vertex_map = {old_index: new_index for old_index, vertex_tuple in enumerate(vertices_as_tuples) for new_index, sorted_vertex_tuple in enumerate(sorted_vertices_as_tuples) if vertex_tuple == sorted_vertex_tuple} \n",
    "    reindexed_faces = [[vertex_map[face[0]], vertex_map[face[1]], vertex_map[face[2]]] for face in faces] \n",
    "    sorted_faces = [sorted(sub_arr) for sub_arr in reindexed_faces]   \n",
    "    return np.array(sorted_vertices), np.array(sorted_faces)\n",
    " \n",
    " \n",
    "\n",
    "def augment_mesh(vertices, scale_factor):     \n",
    "    jitter_factor=0.01 \n",
    "    possible_values = np.arange(-jitter_factor, jitter_factor , 0.0005) \n",
    "    offsets = np.random.choice(possible_values, size=vertices.shape) \n",
    "    vertices = vertices + offsets   \n",
    "    \n",
    "    vertices = vertices * scale_factor \n",
    "    # To ensure that the mesh models are on the \"ground\"\n",
    "    min_y = np.min(vertices[:, 1])  \n",
    "    difference = -0.95 - min_y \n",
    "    vertices[:, 1] += difference\n",
    "    return vertices\n",
    "\n",
    "\n",
    "def get_image(file_path):\n",
    "    with PIL.Image.open(file_path) as img:\n",
    "        return torch.from_numpy(np.array(img.convert('RGB'))).float()\n",
    "\n",
    "\n",
    "#load_shapenet(\"./shapenet\", \"./shapenet_csv_files\", 10, 10)   \n",
    "#Find the csv files with the labels in the ShapeNetCore.v1.zip, download at  https://huggingface.co/datasets/ShapeNet/ShapeNetCore-archive  \n",
    "def load_shapenet(directory, per_category, variations ):\n",
    "    obj_datas = []   \n",
    "    chosen_models_count = {}    \n",
    "    print(f\"per_category: {per_category} variations {variations}\")\n",
    "    \n",
    "    with open('shapenet_labels.json' , 'r') as f:\n",
    "        id_info = json.load(f) \n",
    "    \n",
    "    possible_values = np.arange(0.75, 1.0 , 0.005) \n",
    "    scale_factors = np.random.choice(possible_values, size=variations) \n",
    "    \n",
    "    for category in os.listdir(directory): \n",
    "        category_path = os.path.join(directory, category)   \n",
    "        if os.path.isdir(category_path) == False:\n",
    "            continue \n",
    "        \n",
    "        num_files_in_category = len(os.listdir(category_path))\n",
    "        print(f\"{category_path} got {num_files_in_category} files\") \n",
    "        chosen_models_count[category] = 0  \n",
    "        \n",
    "        for filename in os.listdir(category_path):\n",
    "            if filename.endswith((\".obj\", \".glb\", \".off\")):\n",
    "                file_path = os.path.join(category_path, filename)\n",
    "                \n",
    "                if chosen_models_count[category] >= per_category:\n",
    "                    break \n",
    "                if os.path.getsize(file_path) >  20 * 1024: # 20 kb limit = less then 400-600 faces\n",
    "                    continue \n",
    "                if filename[:-4] not in id_info:\n",
    "                    print(\"Unable to find id info for \", filename)\n",
    "                    continue \n",
    "                vertices, faces = get_mesh(file_path) \n",
    "                if len(faces) > 800: \n",
    "                    continue\n",
    "                \n",
    "                chosen_models_count[category] += 1  \n",
    "                textName = id_info[filename[:-4]]   \n",
    "                \n",
    "                face_edges =  derive_face_edges_from_faces(faces)  \n",
    "                for scale_factor in scale_factors: \n",
    "                    aug_vertices = augment_mesh(vertices.copy(), scale_factor)   \n",
    "                    obj_data = {\"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"mps\"), \"faces\":  torch.tensor(faces.tolist(), dtype=torch.long).to(\"mps\"), \"face_edges\" : face_edges, \"texts\": textName }  \n",
    "                    obj_datas.append(obj_data)\n",
    "                    \n",
    "    print(\"=\"*25)\n",
    "    print(\"Chosen models count for each category:\")\n",
    "    for category, count in chosen_models_count.items():\n",
    "        print(f\"{category}: {count}\") \n",
    "    total_chosen_models = sum(chosen_models_count.values())\n",
    "    print(f\"Total number of chosen models: {total_chosen_models}\")\n",
    "    return obj_datas\n",
    "\n",
    "  \n",
    "   \n",
    "def load_filename(directory, variations):\n",
    "    obj_datas = []    \n",
    "    possible_values = np.arange(0.75, 1.0 , 0.005) \n",
    "    scale_factors = np.random.choice(possible_values, size=variations) \n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith((\".obj\", \".glb\", \".off\")): \n",
    "            file_path = os.path.join(directory, filename) \n",
    "            vertices, faces = get_mesh(file_path)  \n",
    "            \n",
    "            faces = torch.tensor(faces.tolist(), dtype=torch.long).to(\"mps\")\n",
    "            face_edges =  derive_face_edges_from_faces(faces)  \n",
    "            texts, ext = os.path.splitext(filename)     \n",
    "            \n",
    "            png_filename = filename.replace('.glb', '.png')\n",
    "            png_file_path = os.path.join(directory, png_filename)\n",
    "            img = None\n",
    "            if os.path.isfile(png_file_path):\n",
    "                img = get_image(png_file_path)\n",
    "            \n",
    "            for scale_factor in scale_factors: \n",
    "                aug_vertices = augment_mesh(vertices.copy(), scale_factor)  \n",
    "                obj_data = {\"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float), \"faces\":  faces, \"face_edges\" : face_edges, \"texts\": texts}\n",
    "                if img is not None:\n",
    "                    obj_data[\"images\"] = img\n",
    "                obj_datas.append(obj_data)\n",
    "                     \n",
    "    print(f\"[create_mesh_dataset] Returning {len(obj_data)} meshes\")\n",
    "    return obj_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip,json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Instruction to download objverse meshes: https://github.com/MarcusLoppe/Objaverse-downloader/tree/main\n",
    "def load_objverse(directory, variations ):\n",
    "    obj_datas = []     \n",
    "    id_info = {}   \n",
    "    pali_captions = pd.read_csv('.\\pali_captions.csv', sep=';') # https://github.com/google-deepmind/objaverse_annotations/blob/main/pali_captions.csv\n",
    "    pali_captions_dict = pali_captions.set_index(\"object_uid\").to_dict()[\"top_aggregate_caption\"]  \n",
    "        \n",
    "    possible_values = np.arange(0.75, 1.0) \n",
    "    scale_factors = np.random.choice(possible_values, size=variations) \n",
    "    \n",
    "    for folder in os.listdir(directory):  \n",
    "        full_folder_path = os.path.join(directory, folder)   \n",
    "        if os.path.isdir(full_folder_path) == False:\n",
    "            continue    \n",
    "         \n",
    "        for filename in tqdm(os.listdir(full_folder_path)):  \n",
    "            if filename.endswith((\".obj\", \".glb\", \".off\")):\n",
    "                file_path = os.path.join(full_folder_path, filename)\n",
    "                kb = os.path.getsize(file_path)  / 1024 \n",
    "                if kb < 1 or kb > 30:\n",
    "                    continue\n",
    "                  \n",
    "                if filename[:-4] not in pali_captions_dict: \n",
    "                    continue   \n",
    "                textName =  pali_captions_dict[filename[:-4]]\n",
    "                try:    \n",
    "                    vertices, faces = get_mesh(file_path)   \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "                \n",
    "                if len(faces) > 250 or len(faces) < 50: \n",
    "                    continue\n",
    "                \n",
    "                faces = torch.tensor(faces.tolist(), dtype=torch.long).to(\"mps\")\n",
    "                face_edges = derive_face_edges_from_faces(faces)   \n",
    "                for scale_factor in scale_factors: \n",
    "                    aug_vertices = augment_mesh(vertices.copy(), scale_factor)   \n",
    "                    obj_data = {\"filename\": filename, \"vertices\": torch.tensor(aug_vertices.tolist(), dtype=torch.float).to(\"mps\"), \"faces\":  faces, \"face_edges\" : face_edges, \"texts\": textName }   \n",
    "                    obj_datas.append(obj_data)  \n",
    "    return obj_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MeshDataset] Loaded 450 entries\n",
      "[MeshDataset] Created from 450 entries\n",
      "dict_keys(['vertices', 'faces', 'face_edges', 'texts', 'images'])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import gc     \n",
    "import os\n",
    "from meshgpt_pytorch import MeshDataset \n",
    " \n",
    "project_name = \"demo_mesh\" \n",
    "\n",
    "working_dir = f'{project_name}'\n",
    "\n",
    "working_dir = Path(working_dir)\n",
    "working_dir.mkdir(exist_ok = True, parents = True)\n",
    "dataset_path = working_dir / (project_name + \".npz\")\n",
    "\n",
    "torch.set_default_device(\"mps\")\n",
    "if not os.path.isfile(dataset_path):\n",
    "    data = load_filename(\"./demo_mesh\",50)  \n",
    "    dataset = MeshDataset(data) \n",
    "    dataset.generate_face_edges()  \n",
    "    dataset.save(dataset_path)\n",
    " \n",
    "dataset = MeshDataset.load(dataset_path) \n",
    "print(dataset.data[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect imported meshes (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "working_dir = '/Users/matteo/Desktop/Teo/Projects/ai_3d_project/meshgpt-pytorch/out'\n",
    "folder = working_dir + '/renders' \n",
    "obj_file_path = Path(folder)\n",
    "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
    "   \n",
    "all_vertices = []\n",
    "all_faces = []\n",
    "vertex_offset = 0\n",
    "translation_distance = 0.5  \n",
    "\n",
    "for r, item in enumerate(data): \n",
    "    \n",
    "    vertices_copy =  np.copy(item['vertices'].cpu())\n",
    "    vertices_copy += translation_distance * (r / 0.2 - 1) \n",
    "    \n",
    "    for vert in vertices_copy:\n",
    "        vertex = vert #vert.to('cpu')\n",
    "        all_vertices.append(f\"v {float(vertex[0])}  {float(vertex[1])}  {float(vertex[2])}\\n\") \n",
    "    for face in item['faces']:\n",
    "        all_faces.append(f\"f {face[0]+1+ vertex_offset} {face[1]+ 1+vertex_offset} {face[2]+ 1+vertex_offset}\\n\")  \n",
    "    vertex_offset = len(all_vertices)\n",
    " \n",
    "obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
    " \n",
    "obj_file_path = f'{folder}/3d_models_inspect.obj' \n",
    "with open(obj_file_path, \"w\") as file:\n",
    "    file.write(obj_file_content)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 50.7M\n"
     ]
    }
   ],
   "source": [
    "autoencoder = MeshAutoencoder(      \n",
    "        decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,   \n",
    "        codebook_size = 2048,  # Smaller vocab size will speed up the transformer training, however if you are training on meshes more then 250 triangle, I'd advice to use 16384 codebook size\n",
    "        dim_codebook = 192,  \n",
    "        dim_area_embed = 16,\n",
    "        dim_coor_embed = 16, \n",
    "        dim_normal_embed = 16,\n",
    "        dim_angle_embed = 8,\n",
    "    \n",
    "    attn_decoder_depth  = 4,\n",
    "    attn_encoder_depth = 2\n",
    ").to(\"mps\")     \n",
    "total_params = sum(p.numel() for p in autoencoder.parameters()) \n",
    "total_params = f\"{total_params / 1000000:.1f}M\"\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n"
     ]
    }
   ],
   "source": [
    "dataset.data = [dict(d) for d in dataset.data] * 10\n",
    "print(len(dataset.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load previous saved model if you had to restart session*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(str(f'{working_dir}/mesh-encoder_{project_name}.pt')) \n",
    "autoencoder.load_state_dict(pkg['model'])\n",
    "for param in autoencoder.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train to about 0.3 loss if you are using a small dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/480:   0%|                                                                                                                                                                                                                                                                                                                                                                                     | 0/281 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected a 'mps:0' generator device but found 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m      7\u001b[0m autoencoder_trainer \u001b[38;5;241m=\u001b[39m MeshAutoencoderTrainer(model\u001b[38;5;241m=\u001b[39mautoencoder,\n\u001b[1;32m      8\u001b[0m                                              warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m      9\u001b[0m                                              dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m                                              learning_rate \u001b[38;5;241m=\u001b[39m learning_rate,\n\u001b[1;32m     14\u001b[0m                                              checkpoint_every_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# autoencoder_trainer.load('./checkpoints/mesh-autoencoder.ckpt.epoch_0_avg_loss_7.35620_recon_4.7973_commit_12.7944.pt')\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_at_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m        \n",
      "File \u001b[0;32m~/Projects/meshgpt-pytorch-kite/meshgpt_pytorch/trainer.py:337\u001b[0m, in \u001b[0;36mMeshAutoencoderTrainer.train\u001b[0;34m(self, num_epochs, stop_at_loss, display_graph)\u001b[0m\n\u001b[1;32m    334\u001b[0m total_epoch_loss, total_epoch_recon_loss, total_epoch_commit_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    336\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader))\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m    338\u001b[0m     is_last \u001b[38;5;241m=\u001b[39m (batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    339\u001b[0m     maybe_no_sync \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n",
      "File \u001b[0;32m~/Projects/meshgpt-pytorch-kite/.venv/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/meshgpt-pytorch-kite/.venv/lib/python3.9/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/meshgpt-pytorch-kite/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Projects/meshgpt-pytorch-kite/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/Projects/meshgpt-pytorch-kite/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:621\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/meshgpt-pytorch-kite/.venv/lib/python3.9/site-packages/torch/utils/data/sampler.py:280\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mnext\u001b[39m(sampler_iter) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)]\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/meshgpt-pytorch-kite/.venv/lib/python3.9/site-packages/torch/utils/data/sampler.py:280\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msampler_iter\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)]\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/meshgpt-pytorch-kite/.venv/lib/python3.9/site-packages/torch/utils/data/sampler.py:167\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n):\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandperm(n, generator\u001b[38;5;241m=\u001b[39mgenerator)\u001b[38;5;241m.\u001b[39mtolist()[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m%\u001b[39m n]\n",
      "File \u001b[0;32m~/Projects/meshgpt-pytorch-kite/.venv/lib/python3.9/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected a 'mps:0' generator device but found 'cpu'"
     ]
    }
   ],
   "source": [
    "batch_size=16 # The batch size should be max 64.\n",
    "grad_accum_every = 4\n",
    "# So set the maximal batch size (max 64) that your VRAM can handle and then use grad_accum_every to create a effective batch size of 64, e.g  16 * 4 = 64\n",
    "learning_rate = 1e-3 # Start with 1e-3 then at staggnation around 0.35, you can lower it to 1e-4.\n",
    "\n",
    "autoencoder.commit_loss_weight = 0.2 # Set dependant on the dataset size, on smaller datasets, 0.1 is fine, otherwise try from 0.25 to 0.4.\n",
    "autoencoder_trainer = MeshAutoencoderTrainer(model=autoencoder,\n",
    "                                             warmup_steps=10, \n",
    "                                             dataset=dataset,\n",
    "                                             num_train_steps=100,\n",
    "                                             batch_size=batch_size,\n",
    "                                             grad_accum_every = grad_accum_every,\n",
    "                                             learning_rate = learning_rate,\n",
    "                                             checkpoint_every_epoch=1) \n",
    "# autoencoder_trainer.load('./checkpoints/mesh-autoencoder.ckpt.epoch_0_avg_loss_7.35620_recon_4.7973_commit_12.7944.pt')\n",
    "loss = autoencoder_trainer.train(480, stop_at_loss = 0.2, display_graph= True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_trainer.save(f'{working_dir}/mesh-encoder_{project_name}.pt')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect how the autoencoder can encode and then provide the decoder with the codes to reconstruct the mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "from meshgpt_pytorch import mesh_render \n",
    "\n",
    "min_mse, max_mse = float('inf'), float('-inf')\n",
    "min_coords, min_orgs, max_coords, max_orgs = None, None, None, None\n",
    "random_samples, random_samples_pred, all_random_samples = [], [], []\n",
    "total_mse, sample_size = 0.0, 200\n",
    "\n",
    "random.shuffle(dataset.data)\n",
    "\n",
    "for item in tqdm(dataset.data[:sample_size]):\n",
    "    codes = autoencoder.tokenize(vertices=item['vertices'], faces=item['faces'], face_edges=item['face_edges']) \n",
    "    \n",
    "    codes = codes.flatten().unsqueeze(0)\n",
    "    codes = codes[:, :codes.shape[-1] // autoencoder.num_quantizers * autoencoder.num_quantizers] \n",
    " \n",
    "    coords, mask = autoencoder.decode_from_codes_to_faces(codes)\n",
    "    orgs = item['vertices'][item['faces']].unsqueeze(0)\n",
    "\n",
    "    mse = torch.mean((orgs.view(-1, 3).cpu() - coords.view(-1, 3).cpu())**2)\n",
    "    total_mse += mse \n",
    "\n",
    "    if mse < min_mse: min_mse, min_coords, min_orgs = mse, coords, orgs\n",
    "    if mse > max_mse: max_mse, max_coords, max_orgs = mse, coords, orgs\n",
    " \n",
    "    if len(random_samples) <= 30:\n",
    "        random_samples.append(coords)\n",
    "        random_samples_pred.append(orgs)\n",
    "    else:\n",
    "        all_random_samples.extend([random_samples_pred, random_samples])\n",
    "        random_samples, random_samples_pred = [], []\n",
    "\n",
    "print(f'MSE AVG: {total_mse / sample_size:.10f}, Min: {min_mse:.10f}, Max: {max_mse:.10f}')    \n",
    "mesh_render.combind_mesh_with_rows(f'{working_dir}\\mse_rows.obj', all_random_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & fine-tuning\n",
    "\n",
    "**Pre-train:** Train the transformer on the full dataset with all the augmentations, the longer / more epochs will create a more robust model.<br/>\n",
    "\n",
    "**Fine-tune:** Since it will take a long time to train on all the possible augmentations of the meshes, I recommend that you remove all the augmentations so you are left with x1 model per mesh.<br/>\n",
    "Below is the function **filter_dataset** that will return a single copy of each mesh.<br/>\n",
    "The function can also check for duplicate labels, this may speed up the fine-tuning process (not recommanded) however this most likely will remove it's ability for novel mesh generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()   \n",
    "max_seq = max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)  * (autoencoder.num_vertices_per_face * autoencoder.num_quantizers) \n",
    "print(\"Max token sequence:\" , max_seq)  \n",
    "\n",
    "conditioner = MultiModalEmbeddingReturner(\n",
    "    condition_on_text = True,\n",
    "    condition_on_image = True,\n",
    "    clip_model_name = 'openai/clip-vit-large-patch14-336')\n",
    "\n",
    "# GPT2-Small model\n",
    "transformer = MeshTransformer(\n",
    "    autoencoder,\n",
    "    conditioner,\n",
    "    dim = 768,\n",
    "    coarse_pre_gateloop_depth = 3,  \n",
    "    fine_pre_gateloop_depth= 3,  \n",
    "    attn_depth = 12,  \n",
    "    attn_heads = 12,  \n",
    "    max_seq_len = max_seq, \n",
    "    condition_on_text = True,\n",
    "    condition_on_image = True,\n",
    "    gateloop_use_heinsen = False,\n",
    "    dropout  = 0.0\n",
    ") \n",
    "\n",
    "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
    "total_params = f\"{total_params / 1000000:.1f}M\"\n",
    "print(f\"Decoder total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset, unique_labels = False):\n",
    "    unique_dicts = []\n",
    "    unique_tensors = set()\n",
    "    texts = set()\n",
    "    for d in dataset.data:\n",
    "        tensor = d[\"faces\"]\n",
    "        tensor_tuple = tuple(tensor.cpu().numpy().flatten())\n",
    "        if unique_labels and d['texts'] in texts:\n",
    "            continue\n",
    "        if tensor_tuple not in unique_tensors:\n",
    "            unique_tensors.add(tensor_tuple)\n",
    "            unique_dicts.append(d)\n",
    "            texts.add(d['texts'])\n",
    "    return unique_dicts \n",
    "#dataset.data = filter_dataset(dataset.data, unique_labels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
    "\n",
    "**If you don't;** <br>\n",
    "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
    "<br>\n",
    "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
    "\n",
    "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(item[\"texts\"] for item in dataset.data))\n",
    "dataset.embed_texts(conditioner, batch_size = 25)\n",
    "dataset.embed_images(conditioner, batch_size = 25)\n",
    "dataset.generate_codes(autoencoder, batch_size = 50)\n",
    "print(dataset.data[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load previous saved model if you had to restart session*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(str(f'{working_dir}\\mesh-transformer_{project_name}.pt')) \n",
    "transformer.load_state_dict(pkg['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # Max 64\n",
    "grad_accum_every = 16\n",
    "\n",
    "# Set the maximal batch size (max 64) that your VRAM can handle and then use grad_accum_every to create a effective batch size of 64, e.g  4 * 16 = 64\n",
    "learning_rate = 1e-2 # Start training with the learning rate at 1e-2 then lower it to 1e-3 at stagnation or at 0.5 loss.\n",
    "\n",
    "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,num_train_steps=100, dataset = dataset,\n",
    "                                 grad_accum_every=grad_accum_every,\n",
    "                                 learning_rate = learning_rate,\n",
    "                                 batch_size=batch_size,\n",
    "                                 checkpoint_every_epoch = 1,\n",
    "                                 # FP16 training, it doesn't speed up very much but can increase the batch size which will in turn speed up the training.\n",
    "                                 # However it might cause nan after a while.\n",
    "                                 # accelerator_kwargs = {\"mixed_precision\" : \"fp16\"}, optimizer_kwargs = { \"eps\": 1e-7} \n",
    "                                 )\n",
    "loss = trainer.train(300, stop_at_loss = 0.005)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(f'{working_dir}\\mesh-transformer_{project_name}.pt')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and view mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using only text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from meshgpt_pytorch import mesh_render \n",
    "from pathlib import Path\n",
    " \n",
    "folder = working_dir / 'renders'\n",
    "obj_file_path = Path(folder)\n",
    "obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
    " \n",
    "text_coords = [] \n",
    "for text in labels[:10]:\n",
    "    print(f\"Generating {text}\")\n",
    "    faces_coordinates = transformer.generate(texts = [text],  temperature = 0.0) \n",
    "    text_coords.append(faces_coordinates)   \n",
    "mesh_render.combind_mesh(f'{folder}/3d_models_all.obj', text_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text + prompt of tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt with 10% of codes/tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from meshgpt_pytorch import mesh_render \n",
    "folder = working_dir / f'renders/text+codes'\n",
    "obj_file_path = Path(folder)\n",
    "obj_file_path.mkdir(exist_ok = True, parents = True)  \n",
    "\n",
    "token_length_procent = 0.10 \n",
    "codes = []\n",
    "texts = []\n",
    "for label in labels:\n",
    "    for item in dataset.data: \n",
    "        if item['texts'] == label:\n",
    "            tokens = autoencoder.tokenize(\n",
    "                vertices = item['vertices'],\n",
    "                faces = item['faces'],\n",
    "                face_edges = item['face_edges']\n",
    "            ) \n",
    "            num_tokens = int(tokens.shape[0] * token_length_procent)  \n",
    "            texts.append(item['texts']) \n",
    "            codes.append(tokens.flatten()[:num_tokens].unsqueeze(0))  \n",
    "            break\n",
    "        \n",
    "coords = []  \n",
    "for text, prompt in zip(texts, codes): \n",
    "    print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
    "    faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0) \n",
    "    coords.append(faces_coordinates)   \n",
    "    print(obj_file_path)\n",
    "      \n",
    "mesh_render.combind_mesh(f'{folder}/text+prompt_{token_length_procent*100}.obj', coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt with 0% to 80% of tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from meshgpt_pytorch import mesh_render \n",
    " \n",
    "folder = working_dir / f'renders/text+codes_rows'\n",
    "obj_file_path = Path(folder)\n",
    "obj_file_path.mkdir(exist_ok = True, parents = True)   \n",
    "\n",
    "mesh_rows = []\n",
    "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
    "    codes = []\n",
    "    texts = []\n",
    "    for label in labels:\n",
    "        for item in dataset.data: \n",
    "            if item['texts'] == label:\n",
    "                tokens = autoencoder.tokenize(\n",
    "                    vertices = item['vertices'],\n",
    "                    faces = item['faces'],\n",
    "                    face_edges = item['face_edges']\n",
    "                ) \n",
    "                num_tokens = int(tokens.shape[0] * token_length_procent) \n",
    "                \n",
    "                texts.append(item['texts']) \n",
    "                codes.append(tokens.flatten()[:num_tokens].unsqueeze(0))  \n",
    "                break\n",
    "            \n",
    "    coords = []   \n",
    "    for text, prompt in zip(texts, codes):  \n",
    "        print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
    "        faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0) \n",
    "        coords.append(faces_coordinates) \n",
    "         \n",
    "    mesh_rows.append(coords) \n",
    "    mesh_render.combind_mesh(f'{folder}/text+prompt_all_{token_length_procent*100}.obj', coords)\n",
    "    \n",
    "mesh_render.combind_mesh_with_rows(f'{folder}/all.obj', mesh_rows)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just some testing for text embedding similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "texts = list(labels)\n",
    "vectors = [transformer.conditioner.text_models[0].embed_text([text], return_text_encodings = False).cpu().flatten() for text in texts]\n",
    " \n",
    "max_label_length = max(len(text) for text in texts)\n",
    " \n",
    "# Print the table header\n",
    "print(f\"{'Text':<{max_label_length}} |\", end=\" \")\n",
    "for text in texts:\n",
    "    print(f\"{text:<{max_label_length}} |\", end=\" \")\n",
    "print()\n",
    "\n",
    "# Print the similarity matrix as a table with fixed-length columns\n",
    "for i in range(len(texts)):\n",
    "    print(f\"{texts[i]:<{max_label_length}} |\", end=\" \")\n",
    "    for j in range(len(texts)):\n",
    "        # Encode the texts and calculate cosine similarity manually\n",
    "        vector_i = vectors[i]\n",
    "        vector_j = vectors[j]\n",
    "        \n",
    "        dot_product = torch.sum(vector_i * vector_j)\n",
    "        norm_vector1 = torch.norm(vector_i)\n",
    "        norm_vector2 = torch.norm(vector_j)\n",
    "        similarity_score = dot_product / (norm_vector1 * norm_vector2)\n",
    "        \n",
    "        # Print with fixed-length columns\n",
    "        print(f\"{similarity_score.item():<{max_label_length}.4f} |\", end=\" \")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
